{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7dJ_pAj-CakQ"},"outputs":[],"source":["import torch\n","%pip install transformers\n","%pip install accelerate -U\n","from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, pipeline\n","import importlib\n","from importlib import reload\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WAn59GbgC1kX"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xe7Bdtc7CakS"},"outputs":[],"source":["# If you are using Colab\n","dir_path = \"/content/drive/Othercomputers/my_computer/dl-nlp_project_named-entity-recognition/\"\n","module_path = dir_path[9:].replace(\"/\", \".\")\n","# imports\n","data_module = importlib.import_module(module_path + \"data\")\n","load_data = data_module.load_data\n","extract_sentences_and_labels = data_module.extract_sentences_and_labels\n","generate_label_vocab = data_module.generate_label_vocab\n","encode_labels = data_module.encode_labels\n","build_label_to_idx = data_module.build_label_to_idx\n","build_idx_to_label = data_module.build_idx_to_label\n","build_word_to_idx = data_module.build_word_to_idx\n","build_idx_to_word = data_module.build_idx_to_word\n","split_data = data_module.split_data\n","create_data_loaders = data_module.create_data_loaders\n","\n","TRAIN_DATA_PATH = data_module.TRAIN_DATA_PATH\n","TEST_DATA_PATH = data_module.TEST_DATA_PATH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Es5tchFhoeqf"},"outputs":[],"source":["reload(data_module)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w4CclgHACakS"},"outputs":[],"source":["# If you are NOT using colab\n","# dir_path = \"\"\n","# from data_new import (\n","#     prepare_data_pipeline,\n","#     TRAIN_DATA_PATH,\n","#     TEST_DATA_PATH,\n","#     PAD,\n","#     tensor_to_sentences,\n","#     tensor_to_labels,\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Omlf7CDLCakT"},"outputs":[],"source":["train_file_path = dir_path + \"data/train.json\"\n","test_file_path = dir_path + \"data/test.json\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1m6W5d-aCakU"},"outputs":[],"source":["train_data, test_data = load_data(train_file_path, test_file_path)\n","train_sentences, train_raw_labels = extract_sentences_and_labels(train_data)\n","test_sentences, test_raw_labels = extract_sentences_and_labels(test_data)\n","\n","# Generate label vocabulary\n","label_vocab = generate_label_vocab(train_raw_labels + test_raw_labels)\n","\n","# Encode labels pre-transformer\n","train_encoded_labels = encode_labels(train_raw_labels, label_vocab, train_sentences)\n","test_encoded_labels = encode_labels(test_raw_labels, label_vocab, test_sentences)\n","\n","word_to_idx = build_word_to_idx(train_sentences + test_sentences)\n","idx_to_word = build_idx_to_word(word_to_idx)\n","label_to_idx = build_label_to_idx(label_vocab)\n","idx_to_label = build_idx_to_label(label_to_idx)\n","\n","train_sentences, train_labels, val_sentences, val_labels = split_data(\n","    train_sentences, train_encoded_labels\n",")\n","train_data_loader, val_data_loader, test_data_loader = create_data_loaders(\n","    train_sentences,\n","    train_labels,\n","    val_sentences,\n","    val_labels,\n","    test_sentences,\n","    test_encoded_labels,\n","    batch_size=32,\n",")\n","test_labels = test_encoded_labels"]},{"cell_type":"code","source":["task = \"ner\"\n","model_checkpoint = \"distilbert-base-uncased\"\n","batch_size = 16"],"metadata":{"id":"EYXxDdEmPpoM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DatasetDict({\n","#     train: Dataset({\n","#         features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","#         num_rows: 14041\n","#     })\n","#     validation: Dataset({\n","#         features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","#         num_rows: 3250\n","#     })\n","#     test: Dataset({\n","#         features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","#         num_rows: 3453\n","#     })\n","# })\n","# https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb#scrollTo=GWiVUF0jIrIv"],"metadata":{"id":"9p6mEjeUP29c"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptmHlzDOEN5c"},"outputs":[],"source":["def tokenize_and_align_labels(sentences, labels):\n","    tokenized_inputs = tokenizer(sentences, is_split_into_words=True, padding=True)\n","\n","    label_list = []\n","    for i, label in enumerate(labels):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n","        previous_word_idx = None\n","        label_ids = []\n","        for word_idx in word_ids:  # Set the special tokens to -100.\n","            if word_idx is None:\n","                label_ids.append([0 for i in range(len(labels[0][0]))])\n","            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n","                label_ids.append(label[word_idx])\n","            else:\n","                label_ids.append([0 for i in range(len(labels[0][0]))])\n","            previous_word_idx = word_idx\n","        label_list.append(label_ids)\n","\n","    tokenized_inputs[\"labels\"] = label_list\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtsDy9q8iAUt"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = CustomBertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=36)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmB1DMs1DC_D"},"outputs":[],"source":["# Tokenize sentences and align labels\n","tokenized_train_data = tokenize_and_align_labels(train_sentences, train_labels)\n","tokenized_val_data = tokenize_and_align_labels(val_sentences, val_labels)\n","tokenized_test_data = tokenize_and_align_labels(test_sentences, test_labels)\n","tokens = tokenizer.convert_ids_to_tokens(tokenized_train_data[\"input_ids\"][0])\n","labels = tokenized_train_data[\"labels\"][0]\n","print(tokenized_train_data[\"input_ids\"][0])\n","for token, label in zip(tokens[:10], labels[:10]):\n","    print(f\"{label}: {token}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFVZe-752bl0"},"outputs":[],"source":["tokenized_train_data.keys()"]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","\n","class NERDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {\n","            'input_ids': torch.tensor(self.encodings['input_ids'][idx], dtype=torch.long),\n","            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx], dtype=torch.long),\n","            'labels': self.labels[idx]\n","        }\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n"],"metadata":{"id":"hLFofLIo2qrJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = NERDataset(tokenized_train_data, tokenized_train_data['labels'])\n","val_dataset = NERDataset(tokenized_val_data, tokenized_val_data['labels'])\n","test_dataset = NERDataset(tokenized_test_data, tokenized_test_data['labels'])"],"metadata":{"id":"1fhbmsk52yDj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EgTc5U8-eR9O"},"outputs":[],"source":["%pip install evaluate\n","%pip install seqeval\n","import evaluate\n","seqeval = evaluate.load(\"seqeval\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCTxUUbReb_b"},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","def compute_metrics(p, threshold=0.5):\n","    logits, labels = p  # logits are raw scores, not probabilities\n","\n","    # Convert numpy to PyTorch tensor\n","    logits = torch.tensor(logits)\n","    labels = torch.tensor(labels)\n","\n","    # Apply threshold to logits to get binary predictions\n","    predictions = (logits.sigmoid() > threshold).int()\n","\n","    # Use a mask to remove padding tokens (all-zero vectors)\n","    padding_mask = labels.sum(dim=-1) != 0\n","    active_labels = labels[padding_mask]\n","    active_predictions = predictions[padding_mask]\n","\n","    # Compute metrics\n","    f1 = f1_score(active_labels.cpu().numpy(), active_predictions.cpu().numpy(), average='micro')\n","\n","    return {\n","        \"f1\": f1,\n","    }"]},{"cell_type":"code","source":["from transformers import BertForTokenClassification\n","import torch.nn as nn\n","\n","class CustomBertForTokenClassification(BertForTokenClassification):\n","    def __init__(self, config):\n","        super(CustomBertForTokenClassification, self).__init__(config)\n","        self.loss_fct = nn.BCEWithLogitsLoss()\n","\n","    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n","        outputs = super().forward(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n","        logits = outputs[0]\n","\n","        loss = None\n","        if labels is not None:\n","            # print(logits.shape) # [8, 289, 36]\n","            # print(labels.shape) # [8, 289, 36]\n","            # print(attention_mask.shape) # [8, 289]\n","            # Reshape labels and compute loss\n","            active_loss = attention_mask.view(-1) == 1\n","            active_logits = logits.view(-1, 36)\n","            active_labels = labels.view(-1, 36)\n","            # print(active_loss.shape) # [2312]\n","            # print(active_logits.shape) # [2312, 36]\n","            # print(active_labels.shape) # [2312, 36]\n","            loss = self.loss_fct(active_logits[active_loss], active_labels[active_loss].type_as(active_logits))\n","\n","        return (loss, logits) + outputs[2:]\n"],"metadata":{"id":"4Ix-s55E5dal"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RDXpl_00hagW"},"outputs":[],"source":["\n","training_args = TrainingArguments(\n","    output_dir=\"my_model\",\n","    learning_rate=5e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=2,\n","    load_best_model_at_end=True,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    warmup_steps=500,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S75OxieZnIMU"},"outputs":[],"source":["from transformers import pipeline\n","\n","classifier = pipeline(\"ner\", model=\"my_model/checkpoint-326\")\n","classifier(train_sentences[0])"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}