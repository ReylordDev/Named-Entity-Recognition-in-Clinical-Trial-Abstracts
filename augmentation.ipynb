{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_augment = \"Frequency\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import importlib\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    dir_path = (\n",
    "        \"drive/Othercomputers/my_computer/dl-nlp_project_named-entity-recognition/\"\n",
    "    )\n",
    "    # dir_path = \"drive/MyDrive/dl-nlp_project_named-entity-recognition/\"\n",
    "    module_path = dir_path.replace(\"/\", \".\")\n",
    "    # imports\n",
    "    data_module = importlib.import_module(module_path + \"data\")\n",
    "    load_data = data_module.load_data\n",
    "    extract_sentences_and_labels = data_module.extract_sentences_and_labels\n",
    "    generate_label_vocab = data_module.generate_label_vocab\n",
    "    split_data = data_module.split_data\n",
    "\n",
    "else:\n",
    "    dir_path = \"./\"\n",
    "    from data import (\n",
    "        load_data,\n",
    "        extract_sentences_and_labels,\n",
    "        generate_label_vocab,\n",
    "        split_data,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = dir_path + \"data/train.json\"\n",
    "test_file_path = dir_path + \"data/test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_data(train_file_path, test_file_path)\n",
    "train_sentences, train_raw_labels = extract_sentences_and_labels(train_data)\n",
    "test_sentences, test_raw_labels = extract_sentences_and_labels(test_data)\n",
    "\n",
    "# Generate label vocabulary\n",
    "label_vocab = generate_label_vocab(train_raw_labels + test_raw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKEN = \"<SPC>\"\n",
    "\n",
    "\n",
    "class Labels:\n",
    "    def __init__(self, num_classes, names):\n",
    "        super().__init__()\n",
    "        self.names = names\n",
    "        print(self.names)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __getitem__(self, label_vector):\n",
    "        return [self.names[idx] for idx, value in enumerate(label_vector) if value == 1]\n",
    "\n",
    "    def decode(self, label_vector):\n",
    "        return self.__getitem__(label_vector)\n",
    "\n",
    "    def encode(self, names):\n",
    "        indexes = []\n",
    "        for name in names:\n",
    "            index = self.names.index(name)\n",
    "            indexes.append(index)\n",
    "        tensor = torch.zeros(self.num_classes)\n",
    "        for index in indexes:\n",
    "            tensor[index] = 1\n",
    "        return tensor\n",
    "\n",
    "    def tensor2sentence(self, tensor):\n",
    "        return [self.decode(vector) for vector in tensor]\n",
    "\n",
    "\n",
    "ner_labels = Labels(\n",
    "    num_classes=len(label_vocab) + 1, names=label_vocab + [SPECIAL_TOKEN]\n",
    ")\n",
    "id2label = ner_labels.decode\n",
    "label2id = ner_labels.encode\n",
    "ner_labels.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(json_file_path):\n",
    "    with open(json_file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    for entry in data:\n",
    "        for sentence in entry[\"sentences\"]:\n",
    "            tokens = sentence[\"words\"]\n",
    "\n",
    "            entities = sentence[\"entities\"]\n",
    "            labels_list = [torch.zeros(ner_labels.num_classes) for x in tokens]\n",
    "            for label_entity in entities:\n",
    "                start_pos = label_entity[\"start_pos\"]\n",
    "                end_pos = label_entity[\"end_pos\"]\n",
    "                label = label_entity[\"label\"]\n",
    "                label_id = label2id([label]).argmax().item()\n",
    "                for label_index in range(start_pos, end_pos + 1):\n",
    "                    labels_list[label_index][label_id] = 1\n",
    "            sentence[\"tokens\"] = tokens\n",
    "            sentence[\"labels_list\"] = labels_list\n",
    "            sentences.append(sentence)\n",
    "\n",
    "    return [x[\"tokens\"] for x in sentences], [x[\"labels_list\"] for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_labels = extract_sentences(train_file_path)\n",
    "test_sentences, test_labels = extract_sentences(test_file_path)\n",
    "train_sentences, train_labels, val_sentences, val_labels = split_data(\n",
    "    train_sentences, train_labels\n",
    ")\n",
    "\n",
    "print(len(train_sentences), len(train_labels))\n",
    "print(len(val_sentences), len(val_labels))\n",
    "print(len(test_sentences), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_name = f\"{dir_path}data/labels/{label}.json\"\n",
    "if os.path.exists(data_file_name):\n",
    "    with open(data_file_name, \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "else:\n",
    "    data = {\n",
    "        \"sentences\": [],\n",
    "        \"labels_lists\": [],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_abbreviations = {\n",
    "    \"ObjectiveDescription\": \"OD\",\n",
    "    \"Precondition\": \"PC\",\n",
    "    \"RelativeChangeValue\": \"RCV\",\n",
    "    \"DiffGroupAbsValue\": \"DGAV\",\n",
    "    \"NumberPatientsCT\": \"NPC\",\n",
    "    \"AllocationRatio\": \"AR\",\n",
    "    \"DoseValue\": \"DV\",\n",
    "    \"AggregatonMethod\": \"AM\",\n",
    "    \"ResultMeasuredValue\": \"RMV\",\n",
    "    \"SdDevResValue\": \"SDRV\",\n",
    "    \"PvalueDiff\": \"PDiff\",\n",
    "    \"ConfIntervalChangeValue\": \"CICV\",\n",
    "    \"PValueChangeValue\": \"PVCV\",\n",
    "    \"ConfIntervalDiff\": \"CID\",\n",
    "    \"TimePoint\": \"TP\",\n",
    "    \"PercentageAffected\": \"PA\",\n",
    "    \"NumberAffected\": \"NA\",\n",
    "    \"SubGroupDescription\": \"SGD\",\n",
    "    \"MinAge\": \"MA\",\n",
    "    \"Frequency\": \"F\",\n",
    "    \"ObservedResult\": \"OR\",\n",
    "    \"SdDevChangeValue\": \"SDCV\",\n",
    "    \"FinalNumPatientsArm\": \"FNPA\",\n",
    "    \"DoseDescription\": \"DD\",\n",
    "    \"PublicationYear\": \"PY\",\n",
    "    \"SdDevBL\": \"SDBL\",\n",
    "    \"ConclusionComment\": \"CC\",\n",
    "    \"Journal\": \"J\",\n",
    "    \"AvgAge\": \"AA\",\n",
    "    \"AggregationMethod\": \"AM\",\n",
    "    \"NumberPatientsArm\": \"NPA\",\n",
    "    \"CTDesign\": \"CTD\",\n",
    "    \"Author\": \"A\",\n",
    "    \"Title\": \"T\",\n",
    "    \"Country\": \"C\",\n",
    "    \"Drug\": \"D\",\n",
    "}\n",
    "label_unabbreviations = {v: k for k, v in label_abbreviations.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "examples_with_labels = []\n",
    "for sentence, labels_list in zip(train_sentences, train_labels):\n",
    "    new_sentence = sentence\n",
    "    found = False\n",
    "    for i, (token, labels) in enumerate(zip(sentence, labels_list)):\n",
    "        if label in id2label(labels):\n",
    "            found = True\n",
    "            if not new_sentence[i].startswith(\"!!\"):\n",
    "                new_sentence[i] = f\"!!{token}!!\"\n",
    "    if found:\n",
    "        # print(\" \".join(new_sentence))\n",
    "        words = []\n",
    "        for word, labels in zip(sentence, labels_list):\n",
    "            # print(id2label(labels), word)\n",
    "            abbreviated_labels = [\n",
    "                label_abbreviations[label] if label in label_abbreviations else label\n",
    "                for label in id2label(labels)\n",
    "            ]\n",
    "            words.append(f\"{word} {abbreviated_labels}\")\n",
    "        # print(words)\n",
    "        # print()\n",
    "        examples.append(new_sentence)\n",
    "        examples_with_labels.append(words)\n",
    "\n",
    "print(len(examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input_str):\n",
    "    pattern = r\"!![^!]+!!|[-;/\\.]|\\w+|\\S\"\n",
    "    # Find all matches\n",
    "    tokens = re.findall(pattern, input_str)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_markdown_table(array, align: str = None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        array: The array to make into a table. Mush be a rectangular array\n",
    "               (constant width and height).\n",
    "        align: The alignment of the cells : 'left', 'center' or 'right'.\n",
    "    \"\"\"\n",
    "    # make sure every elements are strings\n",
    "    array = [[str(elt) for elt in line] for line in array]\n",
    "    # get the width of each column\n",
    "    widths = [max(len(line[i]) for line in array) for i in range(len(array[0]))]\n",
    "    # make every width at least 3 colmuns, because the separator needs it\n",
    "    widths = [max(w, 3) for w in widths]\n",
    "    # center text according to the widths\n",
    "    array = [[elt.center(w) for elt, w in zip(line, widths)] for line in array]\n",
    "\n",
    "    # separate the header and the body\n",
    "    array_head = array[0]\n",
    "    array = array[1:]\n",
    "\n",
    "    header = \"| \" + \" | \".join(array_head) + \" |\"\n",
    "\n",
    "    # alignment of the cells\n",
    "    align = str(align).lower()  # make sure `align` is a lowercase string\n",
    "    if align == \"none\":\n",
    "        # we are just setting the position of the : in the table.\n",
    "        # here there are none\n",
    "        border_left = \"| \"\n",
    "        border_center = \" | \"\n",
    "        border_right = \" |\"\n",
    "    elif align == \"center\":\n",
    "        border_left = \"|:\"\n",
    "        border_center = \":|:\"\n",
    "        border_right = \":|\"\n",
    "    elif align == \"left\":\n",
    "        border_left = \"|:\"\n",
    "        border_center = \" |:\"\n",
    "        border_right = \" |\"\n",
    "    elif align == \"right\":\n",
    "        border_left = \"| \"\n",
    "        border_center = \":| \"\n",
    "        border_right = \":|\"\n",
    "    else:\n",
    "        raise ValueError(\"align must be 'left', 'right' or 'center'.\")\n",
    "    separator = (\n",
    "        border_left + border_center.join([\"-\" * w for w in widths]) + border_right\n",
    "    )\n",
    "\n",
    "    # body of the table\n",
    "    body = [\"\"] * len(array)  # empty string list that we fill after\n",
    "    for idx, line in enumerate(array):\n",
    "        # for each line, change the body at the correct index\n",
    "        body[idx] = \"| \" + \" | \".join(line) + \" |\"\n",
    "    body = \"\\n\".join(body)\n",
    "\n",
    "    return header + \"\\n\" + separator + \"\\n\" + body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESEARCH DESIGN AND METHODS : During the initiation phase of the DURABLE trial , patients were randomized to a !!twice!! !!-!! !!daily!! lispro mix 75 / 25 ( LM75 / 25 ; 75 % lispro protamine suspension , 25 % lispro ) ( n = 1 , 045 ) or !!daily!! glargine ( GL ) ( n = 1 , 046 ) with continuation of prestudy oral antihyperglycemic drugs .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "index = random.randint(0, len(examples) - 1)\n",
    "print(\" \".join(examples[index]))\n",
    "# print(examples_with_labels[index])\n",
    "print()\n",
    "original_table = []\n",
    "original_table.append([\"Word\", \"Labels (Abrv.)\", \"Labels\"])\n",
    "\n",
    "for labels_str in examples_with_labels[index]:\n",
    "    word = labels_str.split(\" \")[0]\n",
    "    labels = labels_str.split(\" \")[1:]\n",
    "    label_list_str = \"\".join(labels).lstrip(\"['\").rstrip(\"']\")\n",
    "    label_list = (\n",
    "        [label_list_str] if label_list_str.isalpha() else label_list_str.split(\",\")\n",
    "    )\n",
    "    label_list = label_list if label_list != [\"\"] else []\n",
    "    if len(label_list) > 1:\n",
    "        for i, label in enumerate(label_list):\n",
    "            label_list[i] = label.strip(\"'\")\n",
    "    original_table.append(\n",
    "        [\n",
    "            word,\n",
    "            label_list,\n",
    "            [\n",
    "                label_unabbreviations[label]\n",
    "                if label in label_unabbreviations\n",
    "                else label\n",
    "                for label in label_list\n",
    "            ],\n",
    "        ]\n",
    "    )\n",
    "print(make_markdown_table(original_table, align=\"left\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 :       RESEARCH      , Suggestion: [], []\n",
      " 1 :       OUTLINE       , Suggestion: [], []\n",
      " 2 :         AND         , Suggestion: [], []\n",
      " 3 :      APPROACHES     , Suggestion: [], []\n",
      " 4 :          :          , Suggestion: [], []\n",
      " 5 :          In         , Suggestion: [], []\n",
      " 6 :         the         , Suggestion: [], []\n",
      " 7 :     preliminary     , Suggestion: [], []\n",
      " 8 :        phase        , Suggestion: [], []\n",
      " 9 :          of         , Suggestion: [], []\n",
      "10 :         the         , Suggestion: [], []\n",
      "11 :        STABLE       , Suggestion: [], []\n",
      "12 :    investigation    , Suggestion: [], []\n",
      "13 :          ,          , Suggestion: [], []\n",
      "14 :       subjects      , Suggestion: [], []\n",
      "15 :         were        , Suggestion: [], []\n",
      "16 :     categorized     , Suggestion: [], []\n",
      "17 :          to         , Suggestion: [], []\n",
      "18 :         take        , Suggestion: [], []\n",
      "19 :          a          , Suggestion: ['F'], ['Frequency']\n",
      "20 :      !!twice!!      , Suggestion: ['F'], ['Frequency']\n",
      "21 :        !!-!!        , Suggestion: ['F'], ['Frequency']\n",
      "22 :      !!daily!!      , Suggestion: [], []\n",
      "23 :       novolog       , Suggestion: [], []\n",
      "24 :         mix         , Suggestion: [], []\n",
      "25 :          60         , Suggestion: [], []\n",
      "26 :          /          , Suggestion: [], []\n",
      "27 :          40         , Suggestion: [], []\n",
      "28 :          (          , Suggestion: [], []\n",
      "29 :         NM60        , Suggestion: [], []\n",
      "30 :          /          , Suggestion: [], []\n",
      "31 :          40         , Suggestion: [], []\n",
      "32 :          ;          , Suggestion: [], []\n",
      "33 :          60         , Suggestion: [], []\n",
      "34 :          %          , Suggestion: [], []\n",
      "35 :       novolog       , Suggestion: [], []\n",
      "36 :      protamine      , Suggestion: [], []\n",
      "37 :      suspension     , Suggestion: [], []\n",
      "38 :          ,          , Suggestion: [], []\n",
      "39 :          40         , Suggestion: [], []\n",
      "40 :          %          , Suggestion: [], []\n",
      "41 :       novolog       , Suggestion: [], []\n",
      "42 :          )          , Suggestion: [], []\n",
      "43 :          (          , Suggestion: [], []\n",
      "44 :          n          , Suggestion: [], []\n",
      "45 :          =          , Suggestion: ['NPA'], ['NumberPatientsArm']\n",
      "46 :          1          , Suggestion: ['NPA'], ['NumberPatientsArm']\n",
      "47 :          ,          , Suggestion: ['NPA'], ['NumberPatientsArm']\n",
      "48 :         049         , Suggestion: [], []\n",
      "49 :          )          , Suggestion: [], []\n",
      "50 :          or         , Suggestion: ['F'], ['Frequency']\n",
      "51 :      !!daily!!      , Suggestion: [], []\n",
      "52 :       levemir       , Suggestion: [], []\n",
      "53 :          (          , Suggestion: [], []\n",
      "54 :          LV         , Suggestion: [], []\n",
      "55 :          )          , Suggestion: [], []\n",
      "56 :          (          , Suggestion: [], []\n",
      "57 :          n          , Suggestion: [], []\n",
      "58 :          =          , Suggestion: ['NPA'], ['NumberPatientsArm']\n",
      "59 :          1          , Suggestion: ['NPA'], ['NumberPatientsArm']\n",
      "60 :          ,          , Suggestion: ['NPA'], ['NumberPatientsArm']\n",
      "61 :         050         , Suggestion: [], []\n",
      "62 :          )          , Suggestion: [], []\n",
      "63 :      alongside      , Suggestion: ['PC'], ['Precondition']\n",
      "64 :        their        , Suggestion: ['PC'], ['Precondition']\n",
      "65 :         pre         , Suggestion: ['PC'], ['Precondition']\n",
      "66 :          -          , Suggestion: ['PC'], ['Precondition']\n",
      "67 :    investigation    , Suggestion: ['PC'], ['Precondition']\n",
      "68 :         oral        , Suggestion: ['PC'], ['Precondition']\n",
      "69 :     hypoglycemic    , Suggestion: [], []\n",
      "70 :        agents       \n",
      "71 :          .          \n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Frequency'], ['Frequency'], ['Frequency'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['NumberPatientsArm'], ['NumberPatientsArm'], ['NumberPatientsArm'], [], [], ['Frequency'], [], [], [], [], [], [], [], ['NumberPatientsArm'], ['NumberPatientsArm'], ['NumberPatientsArm'], [], [], [], ['Precondition'], ['Precondition'], ['Precondition'], ['Precondition'], ['Precondition'], ['Precondition'], []]\n",
      "\n",
      "|  Id |      Word     |         Labels        |\n",
      "|:--- |:------------- |:--------------------- |\n",
      "|  0  |    RESEARCH   |           []          |\n",
      "|  1  |    OUTLINE    |           []          |\n",
      "|  2  |      AND      |           []          |\n",
      "|  3  |   APPROACHES  |           []          |\n",
      "|  4  |       :       |           []          |\n",
      "|  5  |       In      |           []          |\n",
      "|  6  |      the      |           []          |\n",
      "|  7  |  preliminary  |           []          |\n",
      "|  8  |     phase     |           []          |\n",
      "|  9  |       of      |           []          |\n",
      "|  10 |      the      |           []          |\n",
      "|  11 |     STABLE    |           []          |\n",
      "|  12 | investigation |           []          |\n",
      "|  13 |       ,       |           []          |\n",
      "|  14 |    subjects   |           []          |\n",
      "|  15 |      were     |           []          |\n",
      "|  16 |  categorized  |           []          |\n",
      "|  17 |       to      |           []          |\n",
      "|  18 |      take     |           []          |\n",
      "|  19 |       a       |           []          |\n",
      "|  20 |     twice     |     ['Frequency']     |\n",
      "|  21 |       -       |     ['Frequency']     |\n",
      "|  22 |     daily     |     ['Frequency']     |\n",
      "|  23 |    novolog    |           []          |\n",
      "|  24 |      mix      |           []          |\n",
      "|  25 |       60      |           []          |\n",
      "|  26 |       /       |           []          |\n",
      "|  27 |       40      |           []          |\n",
      "|  28 |       (       |           []          |\n",
      "|  29 |      NM60     |           []          |\n",
      "|  30 |       /       |           []          |\n",
      "|  31 |       40      |           []          |\n",
      "|  32 |       ;       |           []          |\n",
      "|  33 |       60      |           []          |\n",
      "|  34 |       %       |           []          |\n",
      "|  35 |    novolog    |           []          |\n",
      "|  36 |   protamine   |           []          |\n",
      "|  37 |   suspension  |           []          |\n",
      "|  38 |       ,       |           []          |\n",
      "|  39 |       40      |           []          |\n",
      "|  40 |       %       |           []          |\n",
      "|  41 |    novolog    |           []          |\n",
      "|  42 |       )       |           []          |\n",
      "|  43 |       (       |           []          |\n",
      "|  44 |       n       |           []          |\n",
      "|  45 |       =       |           []          |\n",
      "|  46 |       1       | ['NumberPatientsArm'] |\n",
      "|  47 |       ,       | ['NumberPatientsArm'] |\n",
      "|  48 |      049      | ['NumberPatientsArm'] |\n",
      "|  49 |       )       |           []          |\n",
      "|  50 |       or      |           []          |\n",
      "|  51 |     daily     |     ['Frequency']     |\n",
      "|  52 |    levemir    |           []          |\n",
      "|  53 |       (       |           []          |\n",
      "|  54 |       LV      |           []          |\n",
      "|  55 |       )       |           []          |\n",
      "|  56 |       (       |           []          |\n",
      "|  57 |       n       |           []          |\n",
      "|  58 |       =       |           []          |\n",
      "|  59 |       1       | ['NumberPatientsArm'] |\n",
      "|  60 |       ,       | ['NumberPatientsArm'] |\n",
      "|  61 |      050      | ['NumberPatientsArm'] |\n",
      "|  62 |       )       |           []          |\n",
      "|  63 |   alongside   |           []          |\n",
      "|  64 |     their     |           []          |\n",
      "|  65 |      pre      |    ['Precondition']   |\n",
      "|  66 |       -       |    ['Precondition']   |\n",
      "|  67 | investigation |    ['Precondition']   |\n",
      "|  68 |      oral     |    ['Precondition']   |\n",
      "|  69 |  hypoglycemic |    ['Precondition']   |\n",
      "|  70 |     agents    |    ['Precondition']   |\n",
      "|  71 |       .       |           []          |\n",
      "\n",
      "Data submitted\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "input_text = input(\"Input Sentence: \")\n",
    "token_list = tokenize(input_text.strip('\"'))\n",
    "labels_list = [[] for x in token_list]\n",
    "for i, token in enumerate(token_list):\n",
    "    suggesting = i + 1 < len(original_table) and 0 < i + 1\n",
    "    if suggesting:\n",
    "        suggestion = (\n",
    "            f\", Suggestion: {original_table[i + 1][1]}, {original_table[i + 1][2]}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"{i:^3}: {token:^20}{suggestion}\",\n",
    "        )\n",
    "    else:\n",
    "        print(f\"{i:^3}: {token:^20}\")\n",
    "table = []\n",
    "table.append([\"Id\", \"Word\", \"Labels\"])\n",
    "input_label = (\n",
    "    label_abbreviations[label_to_augment]\n",
    "    if label_to_augment in label_abbreviations\n",
    "    else label_to_augment\n",
    ")\n",
    "while len(input_label) > 0:\n",
    "    input_label = input_label.upper()\n",
    "    start_idx = input(f\"Start idx for {input_label}: \")\n",
    "    while start_idx.isalpha():\n",
    "        start_idx = input(f\"Start idx for {input_label}: \")\n",
    "    start_idx = int(start_idx)\n",
    "    end_idx = input(f\"End idx for {input_label}: \")\n",
    "    while end_idx.isalpha():\n",
    "        end_idx = input(f\"End idx for {input_label}: \")\n",
    "    end_idx = int(end_idx)\n",
    "    for i in range(start_idx, end_idx + 1):\n",
    "        labels_list[i].append(\n",
    "            label_unabbreviations[input_label]\n",
    "            if input_label in label_unabbreviations\n",
    "            else input_label\n",
    "        )\n",
    "    input_label = input(\"Label: \")\n",
    "    while input_label.isnumeric():\n",
    "        input_label = input(\"Label: \")\n",
    "\n",
    "token_list = [token.replace(\"!!\", \"\") for token in token_list]\n",
    "print(labels_list, end=\"\\n\\n\")\n",
    "for i, (token, labels) in enumerate(zip(token_list, labels_list)):\n",
    "    labels = list(set(labels))\n",
    "    table.append([i, token, labels])\n",
    "print(make_markdown_table(table, align=\"left\"))\n",
    "print()\n",
    "time.sleep(1)\n",
    "submit = input(\"Hit enter to submit\")\n",
    "if len(submit) == 0:\n",
    "    data[\"sentences\"].append(token_list)\n",
    "    data[\"labels_lists\"].append(labels_list)\n",
    "    print(\"Data submitted\")\n",
    "else:\n",
    "    print(\"Data not submitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/labels/Frequency.json was updated.\n"
     ]
    }
   ],
   "source": [
    "data_file_name = f\"{dir_path}data/labels/{label_to_augment}.json\"\n",
    "with open(data_file_name, \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=2)\n",
    "    print(f\"{data_file_name} was updated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
