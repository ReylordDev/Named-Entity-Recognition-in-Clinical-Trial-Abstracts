{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vmT4k923cqXk"},"outputs":[],"source":["import torch\n","import argparse\n","import numpy as np\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.metrics import f1_score\n","import importlib\n","from importlib import reload"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2974,"status":"ok","timestamp":1693948861371,"user":{"displayName":"Luis Klocke","userId":"15052051274443195697"},"user_tz":-120},"id":"6zkB8Ln2cqXk","outputId":"aefeaf87-8f26-47b4-cc85-5786e47be846"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qJ9S2PdQcqXk"},"outputs":[],"source":["# If you are using Colab\n","dir_path = \"/content/drive/Othercomputers/my_computer/dl-nlp_project_named-entity-recognition/\"\n","module_path = dir_path[9:].replace(\"/\", \".\")\n","# imports\n","data_module = importlib.import_module(module_path + \"data_new\")\n","prepare_data_pipeline = data_module.prepare_data_pipeline\n","TRAIN_DATA_PATH = data_module.TRAIN_DATA_PATH\n","TEST_DATA_PATH = data_module.TEST_DATA_PATH\n","PAD = data_module.PAD\n","tensor_to_sentences = data_module.tensor_to_sentences\n","tensor_to_labels = data_module.tensor_to_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E0DBV1WxfYnM"},"outputs":[],"source":["# If you are NOT using colab\n","# dir_path = \"\"\n","# from data_new import (\n","#     prepare_data_pipeline,\n","#     TRAIN_DATA_PATH,\n","#     TEST_DATA_PATH,\n","#     PAD,\n","#     tensor_to_sentences,\n","#     tensor_to_labels,\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lA59IpJQdVR7"},"outputs":[],"source":["train_file_path = dir_path + \"data/train.json\"\n","test_file_path = dir_path + \"data/test.json\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0Evs2X7cqXl"},"outputs":[],"source":["reload(data_module)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ygJU0mvJkNFM"},"outputs":[],"source":["class SimpleRNNModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx):\n","        super(SimpleRNNModel, self).__init__()\n","\n","        # Embedding layer\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n","\n","        # Simple RNN layer\n","        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n","\n","        # Dense layer\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, text):\n","        # Convert token indices to embeddings\n","        embedded = self.embedding(text)\n","\n","        # Pass embeddings through RNN\n","        rnn_output, _ = self.rnn(embedded)\n","\n","        # Pass RNN output through dense layer\n","        predictions = self.fc(rnn_output)\n","\n","        return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gDRgbnVkcqXm"},"outputs":[],"source":["def train(model, train_loader, optimizer, criterion, device, PAD_IDX):\n","    \"\"\"\n","    Training logic for an epoch\n","    \"\"\"\n","    model.train()\n","\n","    epoch_loss = 0\n","\n","    for batch in train_loader:\n","        sentences = batch[\"sentence\"]\n","        labels = batch[\"label\"]\n","        sentences, labels = sentences.to(device), labels.to(device)\n","\n","        # Zero the gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        predictions = model(sentences)\n","        mask = (sentences != PAD_IDX).float()\n","        mask = mask.unsqueeze(-1).expand_as(labels)\n","\n","        # Compute loss (takes logits as input)\n","        loss = criterion(\n","            predictions,\n","            labels\n","        )\n","        loss = (loss * mask).mean() # mask out 'PAD' tokens\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Update weights\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(train_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def validate(model, val_loader, criterion, device):\n","    \"\"\"\n","    Validation logic with micro-F1 score\n","    \"\"\"\n","    model.eval()\n","\n","    epoch_loss = 0\n","    all_predictions = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            sentences = batch[\"sentence\"]\n","            labels = batch[\"label\"]\n","            sentences, labels = sentences.to(device), labels.to(device)\n","\n","            predictions = model(sentences)\n","            mask = (sentences != PAD_IDX).float()\n","            mask = mask.unsqueeze(-1).expand_as(labels)\n","\n","            # Convert sigmoid outputs to binary labels\n","            binary_predictions = (torch.sigmoid(predictions) >= 0.5).float()\n","\n","            all_predictions.append(binary_predictions.view(-1).cpu().numpy())\n","            all_labels.append(labels.view(-1).cpu().numpy())\n","\n","            # this loss is not really necessary\n","            loss = criterion(\n","                predictions,\n","                labels\n","            )\n","            loss = (loss * mask).mean() # mask out 'PAD' tokens\n","            epoch_loss += loss.item()\n","\n","    # Compute micro-F1 score\n","    micro_f1 = f1_score(\n","        np.hstack(all_labels), np.hstack(all_predictions), average=\"micro\"\n","    )\n","\n","    return epoch_loss / len(val_loader), micro_f1"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1693950183997,"user":{"displayName":"Luis Klocke","userId":"15052051274443195697"},"user_tz":-120},"id":"rbLHh1kIcqXm"},"outputs":[],"source":["def evaluate(model, test_loader, device, idx_to_word, idx_to_label):\n","    \"\"\"\n","    Evaluation logic with micro-F1 score\n","    \"\"\"\n","    model.eval()\n","\n","    epoch_loss = 0\n","    all_predictions = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            sentences = batch[\"sentence\"]\n","            labels = batch[\"label\"]\n","            sentences, labels = sentences.to(device), labels.to(device)\n","\n","            predictions = model(sentences)\n","\n","            # Convert sigmoid outputs to binary labels\n","            binary_predictions = (torch.sigmoid(predictions) >= 0.5).float()\n","\n","            # uncomment this if you want to see the predictions\n","            # sigmoid_predictions = torch.sigmoid(predictions)\n","\n","            # print(sentences.shape) # 32, 181\n","            # decoded_sentences = tensor_to_sentences(sentences, idx_to_word)\n","\n","            # print(labels.shape) # 32, 181, 36\n","            # decoded_labels = tensor_to_labels(labels, idx_to_label)\n","\n","            # print(predictions.shape) # 32, 181, 36\n","            # decoded_predictions = tensor_to_labels(binary_predictions, idx_to_label)\n","\n","            # for i in range(len(decoded_sentences)):\n","            #     sentence = decoded_sentences[i]\n","            #     for j in range(len(sentence)):\n","            #         decoded_word = sentence[j]\n","            #         encoded_word = sentences[i][j]\n","            #         decoded_label = decoded_labels[i][j]\n","            #         encoded_label = labels[i][j].cpu().numpy().tolist()\n","            #         decoded_prediction = decoded_predictions[i][j]\n","            #         encoded_prediction = sigmoid_predictions[i][j].cpu().numpy().tolist()\n","            #         if decoded_word != PAD:\n","            #             print(decoded_word, decoded_label, decoded_prediction)\n","            #             print(f\"Encoded Word: {encoded_word.item()}\")\n","            #             print(f\"Encoded Label:      {encoded_label}\")\n","            #             print(f\"Encoded Prediction: {[round(x, 2) for x in encoded_prediction]}\")\n","            #     print()\n","            # foo()\n","\n","            all_predictions.append(binary_predictions.view(-1).cpu().numpy())\n","            all_labels.append(labels.view(-1).cpu().numpy())\n","\n","    # Compute micro-F1 score\n","    micro_f1 = f1_score(\n","        np.hstack(all_labels), np.hstack(all_predictions), average=\"micro\"\n","    )\n","\n","    return epoch_loss / len(test_loader), micro_f1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1693948683990,"user":{"displayName":"Luis Klocke","userId":"15052051274443195697"},"user_tz":-120},"id":"WUvF9VB-cqXn","outputId":"74c6e9b6-4723-48a7-d5b2-96d965c0bd30"},"outputs":[],"source":["# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lyoT5uAacqXn"},"outputs":[],"source":["(\n","    train_loader,\n","    val_loader,\n","    test_loader,\n","    MAX_LENGTH,\n","    word_to_idx,\n","    idx_to_word,\n","    label_to_idx,\n","    idx_to_label,\n",") = prepare_data_pipeline(train_file_path, test_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":121239,"status":"error","timestamp":1693949404237,"user":{"displayName":"Luis Klocke","userId":"15052051274443195697"},"user_tz":-120},"id":"lovKX5ckcqXo","outputId":"656a220e-5512-4ee7-d4f3-cfbbfcfd2e82"},"outputs":[],"source":["config = {}\n","VOCAB_SIZE = len(word_to_idx)\n","config[\"embedding_dim\"] = 100\n","config[\"hidden_dim\"] = 128\n","config[\"epochs\"] = 100\n","config[\"lr\"] = 0.001\n","OUTPUT_DIM = len(label_to_idx)  # Number of labels\n","PAD_IDX = word_to_idx[PAD]\n","\n","model = SimpleRNNModel(\n","    VOCAB_SIZE, config[\"embedding_dim\"], config[\"hidden_dim\"], OUTPUT_DIM, PAD_IDX\n",")\n","model = model.to(device)\n","criterion = nn.BCEWithLogitsLoss(reduction='none')\n","optimizer = optim.Adam(model.parameters(), config[\"lr\"])\n","\n","for epoch in range(config[\"epochs\"]):\n","    train_loss = train(\n","        model, train_loader, optimizer, criterion, device, PAD_IDX\n","    )\n","    val_loss, micro_f1 = validate(model, val_loader, criterion, device)\n","\n","    print(f\"Epoch: {epoch+1:02}\")\n","    print(f\"\\tTrain Loss: {train_loss:.3f}\")\n","    print(f\"\\t Val. Loss: {val_loss:.3f}\")\n","    print(f\"\\t Micro-F1 Score (Val): {micro_f1:.3f}\")\n","\n","test_micro_f1 = evaluate(model, test_loader, device, idx_to_word, idx_to_label)\n","print(f\"Micro-F1 Score (Test): {test_micro_f1:.3f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":14592,"status":"error","timestamp":1693950204054,"user":{"displayName":"Luis Klocke","userId":"15052051274443195697"},"user_tz":-120},"id":"aF1Au_thiH-_","outputId":"1e859ee2-572b-4bcc-ebf8-ad1eab4112bd"},"outputs":[],"source":["evaluate(model, val_loader, device, idx_to_word, idx_to_label)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.11"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
